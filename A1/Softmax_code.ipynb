{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 946,
     "status": "ok",
     "timestamp": 1607074079739,
     "user": {
      "displayName": "Jung-jae Kim",
      "photoUrl": "",
      "userId": "15824832296156243778"
     },
     "user_tz": -480
    },
    "id": "Omj1sv1rK1IG",
    "outputId": "7bc5dec7-dd8d-4e7a-bad2-e13cd1963209"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader import ConllCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emergingE = ConllCorpusReader('emerging_entities_17-master/','.conll',  ('words', 'pos', 'chunk'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qh6qToBNRro4"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2837,
     "status": "ok",
     "timestamp": 1607074085079,
     "user": {
      "displayName": "Jung-jae Kim",
      "photoUrl": "",
      "userId": "15824832296156243778"
     },
     "user_tz": -480
    },
    "id": "Gw331TN8SViw",
    "outputId": "428ce80f-54be-4983-c5d7-732c34533876"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@paulwalk', 'O'), ('It', 'O'), (\"'s\", 'O'), ('the', 'O'), ('view', 'O'), ('from', 'O'), ('where', 'O'), ('I', 'O'), (\"'m\", 'O'), ('living', 'O'), ('for', 'O'), ('two', 'O'), ('weeks', 'O'), ('.', 'O'), ('Empire', 'B-location'), ('State', 'I-location'), ('Building', 'I-location'), ('=', 'O'), ('ESB', 'B-location'), ('.', 'O'), ('Pretty', 'O'), ('bad', 'O'), ('storm', 'O'), ('here', 'O'), ('last', 'O'), ('evening', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "## Training and testing\n",
    "\n",
    "train_sents = list(emergingE.tagged_sents('wnut17train.conll')) \n",
    "valid_sents = list(emergingE.tagged_sents('emerging.dev.conll'))\n",
    "test_sents = list(emergingE.tagged_sents('emerging.test.conll'))\n",
    "\n",
    "print(train_sents[0])\n",
    "#each tuple contains token, syntactic tag, ner label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JWIshaAS1mTk"
   },
   "outputs": [],
   "source": [
    "# functions of sentence representations for sequence labelling\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 974,
     "status": "ok",
     "timestamp": 1607074092127,
     "user": {
      "displayName": "Jung-jae Kim",
      "photoUrl": "",
      "userId": "15824832296156243778"
     },
     "user_tz": -480
    },
    "id": "AmjwjGB817dp",
    "outputId": "e7729f11-4621-4d42-89f3-6a2566dc8209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels in training data: 13\n"
     ]
    }
   ],
   "source": [
    "# sentence representations for sequence labelling\n",
    "train_sent_tokens = [sent2tokens(s) for s in train_sents]\n",
    "train_labels = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "train_id_2_label = list(set([label for sent in train_labels for label in sent]))\n",
    "train_label_2_id = {label:i for i, label in enumerate(train_id_2_label)}\n",
    "print(\"Number of unique labels in training data:\", len(train_id_2_label))\n",
    "\n",
    "def convert_labels_to_inds(sent_labels, label_2_id):\n",
    "  return [label_2_id[label] for label in sent_labels]\n",
    "\n",
    "train_label_inds = [convert_labels_to_inds(sent_labels, train_label_2_id) for sent_labels in train_labels]\n",
    "\n",
    "test_sent_tokens = [sent2tokens(s) for s in test_sents]\n",
    "test_labels = [sent2labels(s) for s in test_sents]\n",
    "\n",
    "### Test set contains label such as (B-corporation,B-person,B-location), \n",
    "### so we have to separate them into (B-corporation), (B-person) and (B-location)\n",
    "### if not we will encounter key error\n",
    "\n",
    "test_labels_sep = list(([label for sent in test_labels for label in sent]))\n",
    "test_labels_sep = \",\".join(test_labels_sep) \n",
    "test_labels_sep = test_labels_sep.split(\",\") \n",
    "###\n",
    "\n",
    "test_label_inds = [convert_labels_to_inds(test_labels_sep, train_label_2_id) for s in test_labels_sep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "m963KaxfbNbs"
   },
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "\n",
    "# converting tokenized sentence lists to vocabulary indices\n",
    "id_2_word = list(set([token for sent in train_sent_tokens for token in sent])) + [\"<pad>\", \"<unk>\"]\n",
    "word_2_id = {w:i for i,w in enumerate(id_2_word)}\n",
    "\n",
    "def convert_tokens_to_inds(sentence, word_2_id):\n",
    "    return [word_2_id.get(t, word_2_id[\"<unk>\"]) for t in sentence]\n",
    "\n",
    "# padding for windows\n",
    "def pad_sentence_for_window(sentence, window_size, pad_token=\"<pad>\"):\n",
    "    return [pad_token]*window_size + sentence + [pad_token]*window_size \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "v4Rcfpt-c25z"
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NN7BXIZVueq5"
   },
   "outputs": [],
   "source": [
    "# Batching sentences together with a DataLoader\n",
    "\n",
    "def my_collate(data, window_size, word_2_id):\n",
    "    \"\"\"\n",
    "    For some chunk of sentences and labels\n",
    "        -add winow padding\n",
    "        -pad for lengths using pad_sequence\n",
    "        -convert our labels to one-hots\n",
    "        -return padded inputs, one-hot labels, and lengths\n",
    "    \"\"\"\n",
    "    \n",
    "    x_s, y_s = zip(*data)\n",
    "\n",
    "    # deal with input sentences as we've seen\n",
    "    window_padded = [convert_tokens_to_inds(pad_sentence_for_window(sentence, window_size), word_2_id)\n",
    "                                                                                  for sentence in x_s]\n",
    "    # append zeros to each list of token ids in batch so that they are all the same length\n",
    "    padded = nn.utils.rnn.pad_sequence([torch.LongTensor(t) for t in window_padded], batch_first=True)\n",
    "    \n",
    "    # convert labels to one-hots\n",
    "    labels = []\n",
    "    lengths = []\n",
    "    for y in y_s:\n",
    "        lengths.append(len(y))\n",
    "        one_hot = torch.zeros(len(y), len(train_id_2_label))\n",
    "        y = torch.tensor(y)\n",
    "        y = y.unsqueeze(1)\n",
    "        label = one_hot.scatter_(1, y, 1)\n",
    "        labels.append(label)\n",
    "    padded_labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "    \n",
    "    return padded.long(), padded_labels, torch.LongTensor(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SeuW6LtGdiEr"
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "# Shuffle True is good practice for train loaders.\n",
    "# Use functools.partial to construct a partially populated collate function\n",
    "train_loader = DataLoader(list(zip(train_sent_tokens, train_label_inds)), \n",
    "                            batch_size=batch_size, shuffle=True, \n",
    "                            collate_fn=partial(my_collate, window_size=2, word_2_id=word_2_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1057,
     "status": "ok",
     "timestamp": 1607075399761,
     "user": {
      "displayName": "Jung-jae Kim",
      "photoUrl": "",
      "userId": "15824832296156243778"
     },
     "user_tz": -480
    },
    "id": "ffgq3qmg2i6M",
    "outputId": "9e4aeb72-080f-49ee-f109-ea53c6597d39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('inputs',\n",
      " tensor([[14878, 14878,  4923, 11797,  1939,  9188,  5905, 14334, 11289,  9563,\n",
      "          7654,  7312, 11697,  4933, 11886,   738,  1770,  1975,  6561,  1401,\n",
      "         13472, 12751, 10026, 14878, 14878],\n",
      "        [14878, 14878,  3772, 13404,  7387,  9851,  3887,  7848,  4655,  1558,\n",
      "          1106, 14878, 14878,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [14878, 14878,  4923,  1554,  1939,  4452, 10077,  1703, 10096,  4737,\n",
      "         11692, 12071, 11512, 12153,   783, 13421,  5573,  8290, 14878, 14878,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [14878, 14878,  8287, 12870, 13852,  3001,   849,  5544,  4017, 13950,\n",
      "          1690, 10368, 14878, 14878,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0]]),\n",
      " torch.Size([4, 25]))\n",
      "('labels',\n",
      " tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
      " torch.Size([4, 21, 13]))\n",
      "tensor([21,  9, 16, 10])\n"
     ]
    }
   ],
   "source": [
    "for batched_input, batched_labels, batch_lengths in train_loader:\n",
    "    pp.pprint((\"inputs\", batched_input, batched_input.size()))\n",
    "    pp.pprint((\"labels\", batched_labels, batched_labels.size()))\n",
    "    pp.pprint(batch_lengths)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5sAza2qj-WKF"
   },
   "outputs": [],
   "source": [
    "class SoftmaxWordWindowClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A one-layer, binary word-window classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, vocab_size, pad_idx=0):\n",
    "        super(SoftmaxWordWindowClassifier, self).__init__()\n",
    "        \"\"\"\n",
    "        Instance variables.\n",
    "        \"\"\"\n",
    "        self.window_size = 2*config[\"half_window\"]+1\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        self.hidden_dim = config[\"hidden_dim\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "        self.freeze_embeddings = config[\"freeze_embeddings\"]\n",
    "        \n",
    "        \"\"\"\n",
    "        Embedding layer\n",
    "        -model holds an embedding for each layer in our vocab\n",
    "        -sets aside a special index in the embedding matrix for padding vector (of zeros)\n",
    "        -by default, embeddings are parameters (so gradients pass through them)\n",
    "        \"\"\"\n",
    "        self.embed_layer = nn.Embedding(vocab_size, self.embed_dim, padding_idx=pad_idx)\n",
    "        if self.freeze_embeddings:\n",
    "            self.embed_layer.weight.requires_grad = False\n",
    "        \n",
    "        \"\"\"\n",
    "        Hidden layer\n",
    "        -we want to map embedded word windows of dim (window_size+1)*self.embed_dim to a hidden layer.\n",
    "        -nn.Sequential allows you to efficiently specify sequentially structured models\n",
    "            -first the linear transformation is evoked on the embedded word windows\n",
    "            -next the nonlinear transformation tanh is evoked.\n",
    "        \"\"\"\n",
    "        self.hidden_layer = nn.Sequential(nn.Linear(self.window_size*self.embed_dim, \n",
    "                                                    self.hidden_dim), \n",
    "                                          nn.Tanh())\n",
    "        \n",
    "        \"\"\"\n",
    "        Output layer\n",
    "        -we want to map elements of the output layer (of size self.hidden dim) to a number of classes.\n",
    "        \"\"\"\n",
    "        self.output_layer = nn.Linear(self.hidden_dim, self.num_classes)\n",
    "        \n",
    "        \"\"\"\n",
    "        Softmax\n",
    "        -The final step of the softmax classifier: mapping final hidden layer to class scores.\n",
    "        -pytorch has both logsoftmax and softmax functions (and many others)\n",
    "        -since our loss is the negative LOG likelihood, we use logsoftmax\n",
    "        -technically you can take the softmax, and take the log but PyTorch's implementation\n",
    "         is optimized to avoid numerical underflow issues.\n",
    "        \"\"\"\n",
    "        self.log_softmax = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Let B:= batch_size\n",
    "            L:= window-padded sentence length\n",
    "            D:= self.embed_dim\n",
    "            S:= self.window_size\n",
    "            H:= self.hidden_dim\n",
    "            \n",
    "        inputs: a (B, L) tensor of token indices\n",
    "        \"\"\"\n",
    "        B, L = inputs.size()\n",
    "        \n",
    "        \"\"\"\n",
    "        Reshaping.\n",
    "        Takes in a (B, L) LongTensor\n",
    "        Outputs a (B, L~, S) LongTensor\n",
    "        \"\"\"\n",
    "        # Fist, get our word windows for each word in our input.\n",
    "        token_windows = inputs.unfold(1, self.window_size, 1)\n",
    "        _, adjusted_length, _ = token_windows.size()\n",
    "        \n",
    "        # Good idea to do internal tensor-size sanity checks, at the least in comments!\n",
    "        assert token_windows.size() == (B, adjusted_length, self.window_size)\n",
    "        \n",
    "        \"\"\"\n",
    "        Embedding.\n",
    "        Takes in a torch.LongTensor of size (B, L~, S) \n",
    "        Outputs a (B, L~, S, D) FloatTensor.\n",
    "        \"\"\"\n",
    "        embedded_windows = self.embed_layer(token_windows)\n",
    "        \n",
    "        \"\"\"\n",
    "        Reshaping.\n",
    "        Takes in a (B, L~, S, D) FloatTensor.\n",
    "        Resizes it into a (B, L~, S*D) FloatTensor.\n",
    "        -1 argument \"infers\" what the last dimension should be based on leftover axes.\n",
    "        \"\"\"\n",
    "        embedded_windows = embedded_windows.view(B, adjusted_length, -1)\n",
    "        \n",
    "        \"\"\"\n",
    "        Layer 1.\n",
    "        Takes in a (B, L~, S*D) FloatTensor.\n",
    "        Resizes it into a (B, L~, H) FloatTensor\n",
    "        \"\"\"\n",
    "        layer_1 = self.hidden_layer(embedded_windows)\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Layer 2\n",
    "        Takes in a (B, L~, H) FloatTensor.\n",
    "        Resizes it into a (B, L~, 2) FloatTensor.\n",
    "        \"\"\"\n",
    "        output = self.output_layer(layer_1)\n",
    "        \n",
    "        \"\"\"\n",
    "        Softmax.\n",
    "        Takes in a (B, L~, 2) FloatTensor of unnormalized class scores.\n",
    "        Outputs a (B, L~, 2) FloatTensor of (log-)normalized class scores.\n",
    "        \"\"\"\n",
    "        output = self.log_softmax(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6kZIRM_I-gNI"
   },
   "outputs": [],
   "source": [
    "def loss_function(outputs, labels, lengths):\n",
    "    \"\"\"Computes negative LL loss on a batch of model predictions.\"\"\"\n",
    "    B, L, num_classes = outputs.size()\n",
    "    num_elems = lengths.sum().float()\n",
    "        \n",
    "    # get only the values with non-zero labels\n",
    "    loss = outputs*labels\n",
    "    \n",
    "    # rescale average\n",
    "    return -loss.sum() / num_elems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GyATHYB5-5nz"
   },
   "outputs": [],
   "source": [
    "def train_epoch(loss_function, optimizer, model, train_data):\n",
    "    \n",
    "    ## For each batch, we must reset the gradients\n",
    "    ## stored by the model.   \n",
    "    total_loss = 0\n",
    "    for batch, labels, lengths in train_data:\n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        # evoke model in training mode on batch\n",
    "        outputs = model.forward(batch)\n",
    "        # compute loss w.r.t batch\n",
    "        loss = loss_function(outputs, labels, lengths)\n",
    "        # pass gradients back, startiing on loss value\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # return the total to keep track of how you did this time around\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AdCbC3er-9GM"
   },
   "outputs": [],
   "source": [
    "config = {\"batch_size\": 4,\n",
    "          \"half_window\": 2,\n",
    "          \"embed_dim\": 25,\n",
    "          \"hidden_dim\": 25,\n",
    "          \"num_classes\": 13,\n",
    "          \"freeze_embeddings\": False,\n",
    "         }\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 100\n",
    "model = SoftmaxWordWindowClassifier(config, len(word_2_id))\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 977177,
     "status": "ok",
     "timestamp": 1607076391209,
     "user": {
      "displayName": "Jung-jae Kim",
      "photoUrl": "",
      "userId": "15824832296156243778"
     },
     "user_tz": -480
    },
    "id": "CUIPUmdJ_R9U",
    "outputId": "d84068e7-0524-42ce-a90e-b879d84a08b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1984.0911266803741\n",
      "1 1792.0715091228485\n",
      "2 1607.2546564340591\n",
      "3 1428.6124905347824\n",
      "4 1261.417154788971\n",
      "5 1111.3024247288704\n",
      "6 978.8777705430984\n",
      "7 868.2715724110603\n",
      "8 777.5037584900856\n",
      "9 699.8801460266113\n",
      "10 634.8877540230751\n",
      "11 585.3708375096321\n",
      "12 544.7903020083904\n",
      "13 506.13670694828033\n",
      "14 477.1411560624838\n",
      "15 454.2796929627657\n",
      "16 431.78946666419506\n",
      "17 413.5574513077736\n",
      "18 401.9932784512639\n",
      "19 385.9289086908102\n",
      "20 376.22236704826355\n",
      "21 366.0073767527938\n",
      "22 358.61624309420586\n",
      "23 349.6384425610304\n",
      "24 341.78234745562077\n",
      "25 336.6677959486842\n",
      "26 332.50128585100174\n",
      "27 326.40398567169905\n",
      "28 321.82110530883074\n",
      "29 319.4336385950446\n",
      "30 316.42031425237656\n",
      "31 313.945821352303\n",
      "32 309.1180630326271\n",
      "33 307.0945975407958\n",
      "34 301.8980015181005\n",
      "35 302.4247559569776\n",
      "36 300.71974082291126\n",
      "37 298.57036846131086\n",
      "38 297.4774141609669\n",
      "39 295.9250449799001\n",
      "40 293.87659879401326\n",
      "41 294.1891141496599\n",
      "42 291.8307018019259\n",
      "43 290.55560522153974\n",
      "44 290.0176186785102\n",
      "45 288.4694458581507\n",
      "46 290.69103196263313\n",
      "47 285.3020005635917\n",
      "48 286.0992246977985\n",
      "49 285.42470322176814\n",
      "50 283.05480482801795\n",
      "51 283.4681849963963\n",
      "52 281.798903927207\n",
      "53 282.16640519723296\n",
      "54 282.98973498120904\n",
      "55 280.56557754054666\n",
      "56 281.3960764147341\n",
      "57 277.1977654211223\n",
      "58 280.02986623346806\n",
      "59 279.2754381671548\n",
      "60 278.31241361796856\n",
      "61 277.60159132257104\n",
      "62 278.5124855078757\n",
      "63 277.9728636853397\n",
      "64 276.955265507102\n",
      "65 277.472096554935\n",
      "66 276.6942363381386\n",
      "67 274.4087910167873\n",
      "68 274.8529318533838\n",
      "69 273.7909520752728\n",
      "70 276.37995108217\n",
      "71 274.16476432979107\n",
      "72 274.90685304254293\n",
      "73 275.01163578778505\n",
      "74 276.03534434363246\n",
      "75 271.83678248152137\n",
      "76 272.1792895644903\n",
      "77 270.33009542152286\n",
      "78 274.4534385763109\n",
      "79 272.5692033097148\n",
      "80 274.4494830034673\n",
      "81 272.5119951181114\n",
      "82 271.70500018820167\n",
      "83 272.76815248280764\n",
      "84 271.77649813890457\n",
      "85 272.6036176905036\n",
      "86 272.6538703478873\n",
      "87 270.2446316257119\n",
      "88 272.3693539649248\n",
      "89 270.57217145711184\n",
      "90 271.4938544332981\n",
      "91 272.02028929814696\n",
      "92 269.07329988107085\n",
      "93 272.4935442470014\n",
      "94 270.30595829337835\n",
      "95 271.57193425670266\n",
      "96 271.51481158286333\n",
      "97 270.6403106711805\n",
      "98 269.64140623807907\n",
      "99 268.69167917594314\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = train_epoch(loss_function, optimizer, model, train_loader)\n",
    "    print(epoch, epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1rTPZHVRrpz"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Ei4N1XO9czHO"
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(list(zip(test_sent_tokens, test_label_inds)), \n",
    "                            batch_size=batch_size, shuffle=False, \n",
    "                            collate_fn=partial(my_collate, window_size=2, word_2_id=word_2_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QMoQKT9Lc_-H"
   },
   "outputs": [],
   "source": [
    "test_outputs = []\n",
    "for test_instance, labs, _ in test_loader:\n",
    "    outputs_full = model.forward(test_instance)\n",
    "    outputs = torch.argmax(outputs_full, dim=2)\n",
    "    for i in range(outputs.size(0)):\n",
    "      test_outputs.append(outputs[i].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "hN-RvWGZ25xJ"
   },
   "outputs": [],
   "source": [
    "y_test = test_labels\n",
    "y_pred = []\n",
    "for test, pred in zip(test_labels, test_outputs):\n",
    "  y_pred.append([train_id_2_label[id] for id in pred[:len(test)]])\n",
    "\n",
    "assert len(y_pred) == len(y_test), '{} vs. {}'.format(len(y_pred), len(y_test))\n",
    "for i, pred, test in zip(list(range(len(y_pred))), y_pred, y_test):\n",
    "  assert len(pred) == len(test), '{}: {} vs. {}'.format(i, len(pred), len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3591,
     "status": "ok",
     "timestamp": 1607076460340,
     "user": {
      "displayName": "Jung-jae Kim",
      "photoUrl": "",
      "userId": "15824832296156243778"
     },
     "user_tz": -480
    },
    "id": "PdTxgk0Z3EX4",
    "outputId": "3b18e05a-4df5-41c4-f612-605591a8b09c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brand\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9190593176758906"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate CRF model\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=train_id_2_label)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AI6127-2021Spring-week03-tutorial-softmax-classifier-for-NER.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
