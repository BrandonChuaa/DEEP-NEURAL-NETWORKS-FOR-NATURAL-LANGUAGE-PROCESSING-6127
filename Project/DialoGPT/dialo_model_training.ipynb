{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"dialo_model_training.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2da71c31386a4332be5a241008e2d8b0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d9fe6b0dd64e4bd58d9b9922df82d485","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ce59a0a3316747bea3cbf3437f822a2f","IPY_MODEL_a1df765dae0841298cad8503bc98ed78"]}},"d9fe6b0dd64e4bd58d9b9922df82d485":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ce59a0a3316747bea3cbf3437f822a2f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_237d8406da9246069a69903dd9694645","_dom_classes":[],"description":"Epoch: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":5,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":5,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fb9e9e26211b4facb2634bb385c45908"}},"a1df765dae0841298cad8503bc98ed78":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3edb116d89f5454a9ee684cc48661229","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5/5 [3:20:38&lt;00:00, 2407.70s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5804d6eddf1d48489909f62c8a336068"}},"237d8406da9246069a69903dd9694645":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"fb9e9e26211b4facb2634bb385c45908":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3edb116d89f5454a9ee684cc48661229":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5804d6eddf1d48489909f62c8a336068":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8231dd9a15c9440bba085d98b0e3d6ac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_732f05bb101a4b47a133cc04fdfb3782","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a2f00cf483084537a9d17a4b658b97b6","IPY_MODEL_3efef967bffc4de3931dc56eb3622bde"]}},"732f05bb101a4b47a133cc04fdfb3782":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a2f00cf483084537a9d17a4b658b97b6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2e9a08984d1f4a92827cdeadcfc1b8eb","_dom_classes":[],"description":"Iteration: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":23482,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":23482,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_89283918c00d43319c4415f585bba697"}},"3efef967bffc4de3931dc56eb3622bde":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a705fa4b34584b4a9dfaf62116021ad6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 23482/23482 [2:48:59&lt;00:00,  2.32it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_64aa634250e543a2bc26ba5817de3258"}},"2e9a08984d1f4a92827cdeadcfc1b8eb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"89283918c00d43319c4415f585bba697":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a705fa4b34584b4a9dfaf62116021ad6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"64aa634250e543a2bc26ba5817de3258":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dfecd6d773f9498392f9473e2d74cc5a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0452af204dd34bca8ee17f46d6f8359a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0aedbed139d14bf48d3a7337cca7f75b","IPY_MODEL_87ae061950aa475bb743c76236afe82e"]}},"0452af204dd34bca8ee17f46d6f8359a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0aedbed139d14bf48d3a7337cca7f75b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_11728ad235c443aaad447d6820accfd3","_dom_classes":[],"description":"Iteration: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":23482,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":23482,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_11ef9f8394974b0e926e92b3250fa4c3"}},"87ae061950aa475bb743c76236afe82e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_540274aa035c4e888e819b448d529a68","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 23482/23482 [2:08:39&lt;00:00,  3.04it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5fbb58bbc6ef4bd89799e73737f2d3ad"}},"11728ad235c443aaad447d6820accfd3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"11ef9f8394974b0e926e92b3250fa4c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"540274aa035c4e888e819b448d529a68":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5fbb58bbc6ef4bd89799e73737f2d3ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bed6bdc80ac548c89c9bb47cafba0a2a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4277e4bbe9d14444ab2fce96fa9a6e2f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d3fa669401d544c7a9b7bbd4c962dada","IPY_MODEL_06e802e0bdd7486f80fc032a50e74eee"]}},"4277e4bbe9d14444ab2fce96fa9a6e2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d3fa669401d544c7a9b7bbd4c962dada":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_60a519ac383544a89536820ea102cb09","_dom_classes":[],"description":"Iteration: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":23482,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":23482,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5c8b6d19fe85433aaa6d20f48725200e"}},"06e802e0bdd7486f80fc032a50e74eee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_87b881ba9c8048e0b8a1288303e923a3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 23482/23482 [1:28:24&lt;00:00,  4.43it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7a1a14facd044644b91f19cd16e67197"}},"60a519ac383544a89536820ea102cb09":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5c8b6d19fe85433aaa6d20f48725200e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"87b881ba9c8048e0b8a1288303e923a3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7a1a14facd044644b91f19cd16e67197":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9b01125158ed4c01814e1d2bdb13f37d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_81345b633e334522a75b2a9d33319ee0","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d4c3948b20f841b991f9e19ebafbbbf8","IPY_MODEL_b4739800d9434501b245b9379ae44167"]}},"81345b633e334522a75b2a9d33319ee0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d4c3948b20f841b991f9e19ebafbbbf8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6a81419b30e1406fa7fc243d16ee9117","_dom_classes":[],"description":"Iteration: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":23482,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":23482,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_875a22c258864d19be5447cef46a7ed8"}},"b4739800d9434501b245b9379ae44167":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cdb114f242e64856962c3d53d88b7181","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 23482/23482 [48:16&lt;00:00,  8.11it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b72a92bf57754250a06de552bfa01ea0"}},"6a81419b30e1406fa7fc243d16ee9117":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"875a22c258864d19be5447cef46a7ed8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cdb114f242e64856962c3d53d88b7181":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b72a92bf57754250a06de552bfa01ea0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8cd1bfdddccb40ecb4ae44a22189a185":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_933ecb0c1dae4edca7797e6c81f0879c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_eee7c31cef974672a2f6191f4c39e0c7","IPY_MODEL_7448397e291b41b79937651471f222c1"]}},"933ecb0c1dae4edca7797e6c81f0879c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"eee7c31cef974672a2f6191f4c39e0c7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_391fc01895a04018bff4befd3d86dcf5","_dom_classes":[],"description":"Iteration: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":23482,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":23482,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_03749a13585343ee8d0b6725fe39a70f"}},"7448397e291b41b79937651471f222c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_be2b50a80d6c4c3bb0a8e0aca99c8498","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 23482/23482 [39:54&lt;00:00,  9.81it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6a6e555898d742dd8044fa02ec01fd31"}},"391fc01895a04018bff4befd3d86dcf5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"03749a13585343ee8d0b6725fe39a70f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"be2b50a80d6c4c3bb0a8e0aca99c8498":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6a6e555898d742dd8044fa02ec01fd31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"051194706137404d9901ccd5bbd8f22c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d5d0b01b3304400dad336377fa31e67d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3f476748b0d643d89122097494072d0d","IPY_MODEL_04743b86d8bb4dcdb91c5c18b4d9e5f9"]}},"d5d0b01b3304400dad336377fa31e67d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3f476748b0d643d89122097494072d0d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e06bc31a292147d690a450b7aefb82bb","_dom_classes":[],"description":"Evaluating: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":5871,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":5871,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_eedd9491bf714cf49723dd01d3cc584f"}},"04743b86d8bb4dcdb91c5c18b4d9e5f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a43cecf69dbc4c7ebf85acba08b4515e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5871/5871 [01:32&lt;00:00, 63.58it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9a6b8fc61da94576917e87acb660236f"}},"e06bc31a292147d690a450b7aefb82bb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"eedd9491bf714cf49723dd01d3cc584f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a43cecf69dbc4c7ebf85acba08b4515e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9a6b8fc61da94576917e87acb660236f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7a216ce9c8fc4ba9a35eafa52a1f71f5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_650a8c352904453a9a0c9dc36f416742","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e3a475ecc9f54665808defa38734f14f","IPY_MODEL_6f60dd2e60b14ecab30b3f1ad4ccb378"]}},"650a8c352904453a9a0c9dc36f416742":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e3a475ecc9f54665808defa38734f14f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_24a54595d6794032bb174a2a149b6a65","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":641,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":641,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d8d609d7a2a948c39629929f7c468383"}},"6f60dd2e60b14ecab30b3f1ad4ccb378":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4dfeff24009043efa8ca6a5bc490fa7b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 641/641 [00:00&lt;00:00, 2.88kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e5976409fdd44aabb127dddf5cf0c22f"}},"24a54595d6794032bb174a2a149b6a65":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d8d609d7a2a948c39629929f7c468383":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4dfeff24009043efa8ca6a5bc490fa7b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e5976409fdd44aabb127dddf5cf0c22f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8d1b0e3e436747e89d6d36c2aa2529e3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_eb97b859a0984e2aa2c1c4ed45fe2344","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cbf53a556e0d4ebfaf3d2ebd202ed113","IPY_MODEL_1dc72c6435234d3a9e6e74efecd9ce17"]}},"eb97b859a0984e2aa2c1c4ed45fe2344":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cbf53a556e0d4ebfaf3d2ebd202ed113":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_950de89e805f4793a28595bb55e6e4e5","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1042301,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1042301,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_88545d8240e2434cace9d502fc715770"}},"1dc72c6435234d3a9e6e74efecd9ce17":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_23d52bce259847c091f3eefdcd3d91db","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.04M/1.04M [00:00&lt;00:00, 8.07MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6a28136ac2184ace8484d29bc8cb4e9a"}},"950de89e805f4793a28595bb55e6e4e5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"88545d8240e2434cace9d502fc715770":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"23d52bce259847c091f3eefdcd3d91db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6a28136ac2184ace8484d29bc8cb4e9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"06b3f70fae754f88b23a90d494bb6a8d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bacba064088d46f6bbf32b5f77ada358","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_022e099575264c5bafc9f29768eca26c","IPY_MODEL_914a33b2584b4806a6130b5e50040980"]}},"bacba064088d46f6bbf32b5f77ada358":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"022e099575264c5bafc9f29768eca26c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d50cff56c02742d1ba2c5dcc455eac66","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":456318,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":456318,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0c4266003fbb433da5fbce4036d335f1"}},"914a33b2584b4806a6130b5e50040980":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4d55ab5c301c488d847b48d5a4c4406b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 456k/456k [00:00&lt;00:00, 1.76MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c60d1025c0c147c58d2a8ff5d9c6a8fd"}},"d50cff56c02742d1ba2c5dcc455eac66":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0c4266003fbb433da5fbce4036d335f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4d55ab5c301c488d847b48d5a4c4406b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c60d1025c0c147c58d2a8ff5d9c6a8fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a41abc34852f4a77804cb83bfe93c163":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fbd4652998ec402fad312d58fd4ed868","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7e73facec03245ffb29dae52d60ac5c7","IPY_MODEL_8cb607fceb824e45aa07294e2c945ddb"]}},"fbd4652998ec402fad312d58fd4ed868":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7e73facec03245ffb29dae52d60ac5c7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e33b96c69f1345bfae9a0829c47d1868","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":26,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":26,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0c8dcb673d7349029d5402ecc01fcd85"}},"8cb607fceb824e45aa07294e2c945ddb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4a81a436d1b040329c88d514f993851a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 26.0/26.0 [00:00&lt;00:00, 277B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e1fcd4d947fb450986e3caa4614df2f4"}},"e33b96c69f1345bfae9a0829c47d1868":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0c8dcb673d7349029d5402ecc01fcd85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4a81a436d1b040329c88d514f993851a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e1fcd4d947fb450986e3caa4614df2f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"4hyTyW7Le2Sb"},"source":["# TRAINING A DIALOGPT-2 (SMALL) MODEL\n","\n","Details of the [DialoGPT model](https://huggingface.co/transformers/model_doc/dialogpt.html).\n","\n","Code was referenced from [here](https://github.com/chuachinhon/practical_nlp/blob/master/notebooks/3.1_finetune_bot_cch.ipynb).\n","\n","Google Colab was used to train our DialoGPT model but due to the CUDA memory limits, we were only able to build a DialoGPT-small model.\n"]},{"cell_type":"markdown","metadata":{"id":"_tegZ9LqKoqq"},"source":["## 1. MOUNTING YOUR GOOGLE DRIVE (SKIP THIS STEP IF NOT USING GOOGLE COLAB)\n","\n","The code below allows you to mount your google drive that contains the two CSV files (train_df.csv and validate_df.csv) generated by the dialo_data_prep.ipynb notebook."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t989ezrqLdiM","executionInfo":{"status":"ok","timestamp":1619293592056,"user_tz":-480,"elapsed":17068,"user":{"displayName":"Taro Bong","photoUrl":"","userId":"11607109179229239372"}},"outputId":"6fe6e12e-64cc-440b-fa9c-dba4dbcaa29b"},"source":["# For Google Colaboratory\n","import sys, os\n","if 'google.colab' in sys.modules:\n","    # mount google drive\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')\n","    # find automatically the path of the folder containing \"file_name\" :\n","    file_name = 'dialo_model_training.ipynb'\n","    import subprocess\n","    path_to_file = subprocess.check_output('find . -type f -name ' + str(file_name), shell=True).decode(\"utf-8\")\n","    path_to_file = path_to_file.replace(file_name,\"\").replace('\\n',\"\")\n","    # if previous search failed or too long, comment the previous line and simply write down manually the path below :\n","    #path_to_file = '/content/gdrive/My Drive/AI6103_2020_codes/codes/labs_lecture03/lab04_train_vanilla_nn'\n","    print(path_to_file)\n","    # change current path to the folder containing \"file_name\"\n","    os.chdir(path_to_file)\n","    !pwd\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","./gdrive/MyDrive/Dialo/\n","/content/gdrive/MyDrive/Dialo\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"onU41i8g1J3M","executionInfo":{"status":"ok","timestamp":1619293670387,"user_tz":-480,"elapsed":5931,"user":{"displayName":"Taro Bong","photoUrl":"","userId":"11607109179229239372"}},"outputId":"40b8ec82-ee71-4434-865c-89c5fc106b28"},"source":["! pip -q install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 2.1MB 15.4MB/s \n","\u001b[K     |████████████████████████████████| 3.3MB 57.3MB/s \n","\u001b[K     |████████████████████████████████| 901kB 58.2MB/s \n","\u001b[?25h"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g91QzdqU2haO"},"source":["import glob\n","import logging\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import random\n","import re\n","import shutil\n","from typing import Dict, List, Tuple\n","import torch\n","\n","from pathlib import Path\n","\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm.notebook import tqdm, trange\n","from torch.optim import SGD, Adamax\n","\n","from transformers import (\n","    MODEL_WITH_LM_HEAD_MAPPING,\n","    WEIGHTS_NAME,\n","    AdamW,\n","    AutoConfig,\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    PreTrainedModel,\n","    PreTrainedTokenizer,\n","    get_linear_schedule_with_warmup,\n",")\n","\n","try:\n","    from torch.utils.tensorboard import SummaryWriter\n","except ImportError:\n","    from tensorboardX import SummaryWriter\n","\n","# Configs\n","logger = logging.getLogger(__name__)\n","\n","MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IuzSROqxjUKM"},"source":["# 2. MODEL HYPERPARAMETERS / ARGUMENTS"]},{"cell_type":"code","metadata":{"id":"utprDGf06OVt"},"source":["# If you are using a free Colab account, switch to DialoGPT-small\n","# If you encounter GPU out of memory issues on Colab, reduce the batch-size. Here I brought it all the way down\n","# The model checkpoints will take up considerable space on your G-drive. I changed it to save every 20K-steps\n","# If you have more storage space, feel free to adjust the checkpoints interval\n","\n","class Args():\n","    def __init__(self):\n","        self.output_dir = 'output-small' # model output directory\n","        self.cache_dir = 'cached' # cache directory\n","        \n","        # Using DialoGPT-small\n","        self.model_type = 'gpt2'\n","        self.model_name_or_path = 'microsoft/DialoGPT-small'\n","        self.config_name = 'microsoft/DialoGPT-small'\n","        self.tokenizer_name = 'microsoft/DialoGPT-small'\n","        \n","        self.do_train = True\n","        self.do_eval = True\n","        self.num_train_epochs = 5 # number of epochs to train\n","        self.block_size = 64  \n","        self.evaluate_during_training = False # whether to perform evaluation during training\n","        self.per_gpu_train_batch_size = 1 # training batch size, set to small batch sizes if encountering memory issues\n","        self.per_gpu_eval_batch_size = 1  # evaluation batch size, , set to small batch sizes if encountering memory issues\n","        self.adam_epsilon = 1e-8 # AdamW epsilon\n","        self.gradient_accumulation_steps = 1 # number of steps to accumulate gradient\n","        self.learning_rate = 5e-5 # learning rate\n","        self.weight_decay = 0.0 # weight decay     \n","        self.max_grad_norm = 1.0 # gradient clipping\n","        \n","        self.max_steps = -1\n","        self.warmup_steps = 0\n","        self.logging_steps = 20000\n","\n","        self.save_steps = 10000 # number of steps taken before a model checkpoint is saved\n","        self.save_total_limit = 1 # max number of model checkpoints to save. the oldest one will be overwritten by the new checkpoint if reached max number\n","        self.eval_all_checkpoints = False \n","        self.no_cuda = False\n","        self.overwrite_output_dir = True\n","        self.overwrite_cache = True\n","       \n","        self.should_continue = False # whether to continue training from the latest checkpoint\n","        \n","        self.seed = 66\n","        self.local_rank = -1\n","        self.fp16 = False\n","        self.fp16_opt_level = 'O1'\n","\n","args = Args()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"86F3WhnFO4H8"},"source":["## 2.1 CONVERTING CSV DATASET TO FORMAT SUITABLE FOR FEEDING INTO DIALOGPT\n","\n","Converting the CSV dataframes into conversation strings before tokenizing it"]},{"cell_type":"code","metadata":{"id":"PX7jeWpYmOe_"},"source":["# method to flatten the data\n","def construct_conv(row, tokenizer, eos = True):\n","    flatten = lambda l: [item for sublist in l for item in sublist]\n","    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n","    conv = flatten(conv)\n","    return conv\n","\n","class ConversationDataset(Dataset):\n","    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n","\n","        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n","\n","        directory = args.cache_dir\n","        cached_features_file = os.path.join(\n","            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n","        )\n","\n","        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n","            logger.info(\"Loading features from cached file %s\", cached_features_file)\n","            with open(cached_features_file, \"rb\") as handle:\n","                self.examples = pickle.load(handle)\n","        else:\n","            logger.info(\"Creating features from dataset file at %s\", directory)\n","\n","            self.examples = []\n","            for _, row in df.iterrows():\n","                conv = construct_conv(row, tokenizer)\n","                self.examples.append(conv)\n","\n","            logger.info(\"Saving features into cached file %s\", cached_features_file)\n","            with open(cached_features_file, \"wb\") as handle:\n","                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","    def __len__(self):\n","        return len(self.examples)\n","\n","    def __getitem__(self, item):\n","        return torch.tensor(self.examples[item], dtype=torch.long)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"naaRHoXgnStq"},"source":["# cacheing and storing of data/checkpoints\n","\n","def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\n","    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)\n","\n","\n","def set_seed(args):\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    if args.n_gpu > 0:\n","        torch.cuda.manual_seed_all(args.seed)\n","\n","\n","def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n","    ordering_and_checkpoint_path = []\n","\n","    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n","\n","    for path in glob_checkpoints:\n","        if use_mtime:\n","            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n","        else:\n","            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n","            if regex_match and regex_match.groups():\n","                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n","\n","    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n","    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n","    return checkpoints_sorted\n","\n","\n","def _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n","    if not args.save_total_limit:\n","        return\n","    if args.save_total_limit <= 0:\n","        return\n","\n","    # check if need to delete older checkpoint due to max number of checkpoints saved\n","    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n","    if len(checkpoints_sorted) <= args.save_total_limit:\n","        return\n","\n","    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n","    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n","    for checkpoint in checkpoints_to_be_deleted:\n","        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n","        shutil.rmtree(checkpoint)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pkvMNnrnVHQw"},"source":["# 3. TRAINING AND EVALUATION\n","\n"]},{"cell_type":"code","metadata":{"id":"tXzKlXHeu0Mb"},"source":["def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n","    \"\"\" Train the model \"\"\"\n","    if args.local_rank in [-1, 0]:\n","        tb_writer = SummaryWriter()\n","\n","    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n","\n","    def collate(examples: List[torch.Tensor]):\n","        if tokenizer._pad_token is None:\n","            return pad_sequence(examples, batch_first=True)\n","        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n","\n","    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n","    train_dataloader = DataLoader(\n","        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n","    )\n","\n","    if args.max_steps > 0:\n","        t_total = args.max_steps\n","        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n","    else:\n","        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n","\n","    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n","    model.resize_token_embeddings(len(tokenizer))\n","    # add_special_tokens_(model, tokenizer)\n","\n","\n","    # Prepare optimizer and schedule (linear warmup and decay)\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": args.weight_decay,\n","        },\n","        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n","    ]\n","    ######################################################################## Change optimizer here ############################################################################################\n","    #optimizer = SGD(lr=args.learning_rate, weight_decay = 0.1, momentum = 0.98)\n","    #optimizer = Adamax(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n","    ###########################################################################################################################################################################################\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n","    )\n","\n","    # Check if saved optimizer or scheduler states exist\n","    if (\n","        args.model_name_or_path\n","        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n","        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n","    ):\n","        # Load in optimizer and scheduler states\n","        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n","        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n","\n","    if args.fp16:\n","        try:\n","            from apex import amp\n","        except ImportError:\n","            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n","        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n","\n","    # multi-gpu training (should be after apex fp16 initialization)\n","    if args.n_gpu > 1:\n","        model = torch.nn.DataParallel(model)\n","\n","    # Distributed training (should be after apex fp16 initialization)\n","    if args.local_rank != -1:\n","        model = torch.nn.parallel.DistributedDataParallel(\n","            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n","        )\n","\n","    # Train!\n","    logger.info(\"***** Running training *****\")\n","    logger.info(\"  Num examples = %d\", len(train_dataset))\n","    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n","    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n","    logger.info(\n","        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n","        args.train_batch_size\n","        * args.gradient_accumulation_steps\n","        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n","    )\n","    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n","    logger.info(\"  Total optimization steps = %d\", t_total)\n","\n","    global_step = 0\n","    epochs_trained = 0\n","    steps_trained_in_current_epoch = 0\n","    # Check if continuing training from a checkpoint\n","    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n","        try:\n","            # set global_step to global_step of last saved checkpoint from model path\n","            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n","            global_step = int(checkpoint_suffix)\n","            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n","            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n","\n","            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n","            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n","            logger.info(\"  Continuing training from global step %d\", global_step)\n","            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n","        except ValueError:\n","            logger.info(\"  Starting fine-tuning.\")\n","\n","    tr_loss, logging_loss = 0.0, 0.0\n","\n","    model.zero_grad()\n","    train_iterator = trange(\n","        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n","    )\n","    set_seed(args)  # Added here for reproducibility\n","    for _ in train_iterator:\n","        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n","        for step, batch in enumerate(epoch_iterator):\n","\n","            # Skip past any already trained steps if resuming training\n","            if steps_trained_in_current_epoch > 0:\n","                steps_trained_in_current_epoch -= 1\n","                continue\n","\n","            inputs, labels = (batch, batch)\n","            if inputs.shape[1] > 1024: continue\n","            inputs = inputs.to(args.device)\n","            labels = labels.to(args.device)\n","            model.train()\n","            outputs = model(inputs, labels=labels)\n","            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n","\n","            if args.n_gpu > 1:\n","                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n","            if args.gradient_accumulation_steps > 1:\n","                loss = loss / args.gradient_accumulation_steps\n","\n","            if args.fp16:\n","                with amp.scale_loss(loss, optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","\n","            tr_loss += loss.item()\n","            if (step + 1) % args.gradient_accumulation_steps == 0:\n","                if args.fp16:\n","                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n","                else:\n","                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n","                optimizer.step()\n","                scheduler.step()  # Update learning rate schedule\n","                model.zero_grad()\n","                global_step += 1\n","\n","                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n","                    # Log metrics\n","                    if (\n","                        args.local_rank == -1 and args.evaluate_during_training\n","                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n","                        results = evaluate(args, model, tokenizer)\n","                        for key, value in results.items():\n","                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n","                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n","                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n","                    logging_loss = tr_loss\n","\n","                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n","                    checkpoint_prefix = \"checkpoint\"\n","                    # Save model checkpoint\n","                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n","                    os.makedirs(output_dir, exist_ok=True)\n","                    model_to_save = (\n","                        model.module if hasattr(model, \"module\") else model\n","                    )  # Take care of distributed/parallel training\n","                    model_to_save.save_pretrained(output_dir)\n","                    tokenizer.save_pretrained(output_dir)\n","\n","                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n","                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n","\n","                    _rotate_checkpoints(args, checkpoint_prefix)\n","\n","                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n","                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n","                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n","\n","            if args.max_steps > 0 and global_step > args.max_steps:\n","                epoch_iterator.close()\n","                break\n","        if args.max_steps > 0 and global_step > args.max_steps:\n","            train_iterator.close()\n","            break\n","\n","    if args.local_rank in [-1, 0]:\n","        tb_writer.close()\n","\n","    return global_step, tr_loss / global_step\n","\n","# Evaluation of model\n","\n","def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n","    # Loop to handle MNLI double evaluation (matched, mis-matched)\n","    eval_output_dir = args.output_dir\n","\n","    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n","    os.makedirs(eval_output_dir, exist_ok=True)\n","    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n","    # Note that DistributedSampler samples randomly\n","\n","    def collate(examples: List[torch.Tensor]):\n","        if tokenizer._pad_token is None:\n","            return pad_sequence(examples, batch_first=True)\n","        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n","\n","    eval_sampler = SequentialSampler(eval_dataset)\n","    eval_dataloader = DataLoader(\n","        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n","    )\n","\n","    # multi-gpu evaluate\n","    if args.n_gpu > 1:\n","        model = torch.nn.DataParallel(model)\n","\n","    # Eval!\n","    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n","    logger.info(\"  Num examples = %d\", len(eval_dataset))\n","    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n","    eval_loss = 0.0\n","    nb_eval_steps = 0\n","    model.eval()\n","\n","    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","        inputs, labels = (batch, batch)\n","        inputs = inputs.to(args.device)\n","        labels = labels.to(args.device)\n","\n","        with torch.no_grad():\n","            outputs = model(inputs, labels=labels)\n","            lm_loss = outputs[0]\n","            eval_loss += lm_loss.mean().item()\n","        nb_eval_steps += 1\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    perplexity = torch.exp(torch.tensor(eval_loss))\n","\n","    result = {\"perplexity\": perplexity}\n","\n","    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n","    with open(output_eval_file, \"w\") as writer:\n","        logger.info(\"***** Eval results {} *****\".format(prefix))\n","        for key in sorted(result.keys()):\n","            logger.info(\"  %s = %s\", key, str(result[key]))\n","            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n","\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-MGD6bFXV4Z-"},"source":["# main method\n","\n","def main(df_trn, df_val):\n","    args = Args()\n","    \n","    if args.should_continue:\n","        sorted_checkpoints = _sorted_checkpoints(args)\n","        if len(sorted_checkpoints) == 0:\n","            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n","        else:\n","            args.model_name_or_path = sorted_checkpoints[-1]\n","\n","    if (\n","        os.path.exists(args.output_dir)\n","        and os.listdir(args.output_dir)\n","        and args.do_train\n","        and not args.overwrite_output_dir\n","        and not args.should_continue\n","    ):\n","        raise ValueError(\n","            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n","                args.output_dir\n","            )\n","        )\n","\n","    # Setup CUDA, GPU & distributed training\n","    device = torch.device(\"cuda\")\n","    args.n_gpu = torch.cuda.device_count()\n","    args.device = device\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n","    )\n","    logger.warning(\n","        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n","        args.local_rank,\n","        device,\n","        args.n_gpu,\n","        bool(args.local_rank != -1),\n","        args.fp16,\n","    )\n","\n","    # Set seed\n","    set_seed(args)\n","\n","    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n","    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n","    model = AutoModelForCausalLM.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=False,\n","        config=config,\n","        cache_dir=args.cache_dir,\n","    )\n","    model.to(args.device)\n","    \n","    logger.info(\"Training/evaluation parameters %s\", args)\n","\n","    # Training\n","    if args.do_train:\n","        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n","\n","        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n","        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n","\n","    # Using save_pretrained for the model and tokenizer allows you to reload them using from_pretrained()\n","    if args.do_train:\n","        # Create output directory if needed\n","        os.makedirs(args.output_dir, exist_ok=True)\n","\n","        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n","        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","        # They can then be reloaded using `from_pretrained()`\n","        model_to_save = (\n","            model.module if hasattr(model, \"module\") else model\n","        )  # Take care of distributed/parallel training\n","        model_to_save.save_pretrained(args.output_dir)\n","        tokenizer.save_pretrained(args.output_dir)\n","\n","        # Save your training arguments together with the trained model\n","        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n","\n","        # Load a trained model and vocabulary that you have fine-tuned\n","        model = AutoModelForCausalLM.from_pretrained(args.output_dir)\n","        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n","        model.to(args.device)\n","\n","    # Evaluation\n","    results = {}\n","    if args.do_eval and args.local_rank in [-1, 0]:\n","        checkpoints = [args.output_dir]\n","        if args.eval_all_checkpoints:\n","            checkpoints = list(\n","                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n","            )\n","            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n","        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n","        for checkpoint in checkpoints:\n","            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n","            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n","\n","            model = AutoModelForCausalLM.from_pretrained(checkpoint)\n","            model.to(args.device)\n","            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\n","            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n","            results.update(result)\n","\n","    return results"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UZEHDzR0Vjs7"},"source":["## 3.1 TRAIN THE MODEL\n","\n","Takes about 3 to 4 hours to train a model for 5 epochs"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["2da71c31386a4332be5a241008e2d8b0","d9fe6b0dd64e4bd58d9b9922df82d485","ce59a0a3316747bea3cbf3437f822a2f","a1df765dae0841298cad8503bc98ed78","237d8406da9246069a69903dd9694645","fb9e9e26211b4facb2634bb385c45908","3edb116d89f5454a9ee684cc48661229","5804d6eddf1d48489909f62c8a336068","8231dd9a15c9440bba085d98b0e3d6ac","732f05bb101a4b47a133cc04fdfb3782","a2f00cf483084537a9d17a4b658b97b6","3efef967bffc4de3931dc56eb3622bde","2e9a08984d1f4a92827cdeadcfc1b8eb","89283918c00d43319c4415f585bba697","a705fa4b34584b4a9dfaf62116021ad6","64aa634250e543a2bc26ba5817de3258","dfecd6d773f9498392f9473e2d74cc5a","0452af204dd34bca8ee17f46d6f8359a","0aedbed139d14bf48d3a7337cca7f75b","87ae061950aa475bb743c76236afe82e","11728ad235c443aaad447d6820accfd3","11ef9f8394974b0e926e92b3250fa4c3","540274aa035c4e888e819b448d529a68","5fbb58bbc6ef4bd89799e73737f2d3ad","bed6bdc80ac548c89c9bb47cafba0a2a","4277e4bbe9d14444ab2fce96fa9a6e2f","d3fa669401d544c7a9b7bbd4c962dada","06e802e0bdd7486f80fc032a50e74eee","60a519ac383544a89536820ea102cb09","5c8b6d19fe85433aaa6d20f48725200e","87b881ba9c8048e0b8a1288303e923a3","7a1a14facd044644b91f19cd16e67197","9b01125158ed4c01814e1d2bdb13f37d","81345b633e334522a75b2a9d33319ee0","d4c3948b20f841b991f9e19ebafbbbf8","b4739800d9434501b245b9379ae44167","6a81419b30e1406fa7fc243d16ee9117","875a22c258864d19be5447cef46a7ed8","cdb114f242e64856962c3d53d88b7181","b72a92bf57754250a06de552bfa01ea0","8cd1bfdddccb40ecb4ae44a22189a185","933ecb0c1dae4edca7797e6c81f0879c","eee7c31cef974672a2f6191f4c39e0c7","7448397e291b41b79937651471f222c1","391fc01895a04018bff4befd3d86dcf5","03749a13585343ee8d0b6725fe39a70f","be2b50a80d6c4c3bb0a8e0aca99c8498","6a6e555898d742dd8044fa02ec01fd31","051194706137404d9901ccd5bbd8f22c","d5d0b01b3304400dad336377fa31e67d","3f476748b0d643d89122097494072d0d","04743b86d8bb4dcdb91c5c18b4d9e5f9","e06bc31a292147d690a450b7aefb82bb","eedd9491bf714cf49723dd01d3cc584f","a43cecf69dbc4c7ebf85acba08b4515e","9a6b8fc61da94576917e87acb660236f"]},"id":"__iqR8YFV-Ex","executionInfo":{"status":"ok","timestamp":1619308259573,"user_tz":-480,"elapsed":12207569,"user":{"displayName":"Taro Bong","photoUrl":"","userId":"11607109179229239372"}},"outputId":"0c16fc8b-e8b1-4277-a58f-47cd4db7c421"},"source":["train_df = pd.read_csv('data/train_df.csv')\n","validate_df = pd.read_csv('data/validate_df.csv')\n","\n","main(train_df, validate_df)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["04/24/2021 20:27:33 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","04/24/2021 20:27:38 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x7f64384b5e10>\n","04/24/2021 20:27:38 - INFO - __main__ -   Creating features from dataset file at cached\n","04/24/2021 20:28:29 - INFO - __main__ -   Saving features into cached file cached/gpt2_cached_lm_512\n","04/24/2021 20:28:29 - INFO - __main__ -   ***** Running training *****\n","04/24/2021 20:28:29 - INFO - __main__ -     Num examples = 23482\n","04/24/2021 20:28:29 - INFO - __main__ -     Num Epochs = 5\n","04/24/2021 20:28:29 - INFO - __main__ -     Instantaneous batch size per GPU = 1\n","04/24/2021 20:28:29 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 1\n","04/24/2021 20:28:29 - INFO - __main__ -     Gradient Accumulation steps = 1\n","04/24/2021 20:28:29 - INFO - __main__ -     Total optimization steps = 117410\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2da71c31386a4332be5a241008e2d8b0","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch', max=5.0, style=ProgressStyle(description_width='i…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8231dd9a15c9440bba085d98b0e3d6ac","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Iteration', max=23482.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["04/24/2021 20:45:36 - INFO - __main__ -   Saving model checkpoint to output-small/checkpoint-10000\n","04/24/2021 20:45:45 - INFO - __main__ -   Saving optimizer and scheduler states to output-small/checkpoint-10000\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","04/24/2021 21:02:50 - INFO - __main__ -   Saving model checkpoint to output-small/checkpoint-20000\n","04/24/2021 21:02:50 - INFO - __main__ -   Deleting older checkpoint [output-small/checkpoint-10000] due to args.save_total_limit\n","04/24/2021 21:02:54 - INFO - __main__ -   Saving optimizer and scheduler states to output-small/checkpoint-20000\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dfecd6d773f9498392f9473e2d74cc5a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Iteration', max=23482.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["04/24/2021 21:19:55 - INFO - __main__ -   Saving model checkpoint to output-small/checkpoint-30000\n","04/24/2021 21:19:55 - INFO - __main__ -   Deleting older checkpoint [output-small/checkpoint-20000] due to args.save_total_limit\n","04/24/2021 21:20:00 - INFO - __main__ -   Saving optimizer and scheduler states to output-small/checkpoint-30000\n","04/24/2021 21:37:05 - INFO - __main__ -   Saving model checkpoint to output-small/checkpoint-40000\n","04/24/2021 21:37:05 - INFO - __main__ -   Deleting older checkpoint [output-small/checkpoint-30000] due to args.save_total_limit\n","04/24/2021 21:37:14 - INFO - __main__ -   Saving optimizer and scheduler states to output-small/checkpoint-40000\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bed6bdc80ac548c89c9bb47cafba0a2a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Iteration', max=23482.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["04/24/2021 21:54:15 - INFO - __main__ -   Saving model checkpoint to output-small/checkpoint-50000\n","04/24/2021 21:54:15 - INFO - __main__ -   Deleting older checkpoint [output-small/checkpoint-40000] due to args.save_total_limit\n","04/24/2021 21:54:19 - INFO - __main__ -   Saving optimizer and scheduler states to output-small/checkpoint-50000\n","04/24/2021 22:11:20 - INFO - __main__ -   Saving model checkpoint to output-small/checkpoint-60000\n","04/24/2021 22:11:20 - INFO - __main__ -   Deleting older checkpoint [output-small/checkpoint-50000] due to args.save_total_limit\n","04/24/2021 22:11:24 - INFO - __main__ -   Saving optimizer and scheduler states to output-small/checkpoint-60000\n","04/24/2021 22:28:23 - INFO - __main__ -   Saving model checkpoint to output-small/checkpoint-70000\n","04/24/2021 22:28:23 - INFO - __main__ -   Deleting older checkpoint [output-small/checkpoint-60000] due to args.save_total_limit\n","04/24/2021 22:28:27 - INFO - __main__ -   Saving optimizer and scheduler states to output-small/checkpoint-70000\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9b01125158ed4c01814e1d2bdb13f37d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Iteration', max=23482.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["04/24/2021 22:45:26 - INFO - __main__ -   Saving model checkpoint to output-small/checkpoint-80000\n","04/24/2021 22:45:26 - INFO - __main__ -   Deleting older checkpoint [output-small/checkpoint-70000] due to args.save_total_limit\n","04/24/2021 22:45:31 - INFO - __main__ -   Saving optimizer and scheduler states to output-small/checkpoint-80000\n","04/24/2021 23:02:31 - INFO - __main__ -   Saving model checkpoint to output-small/checkpoint-90000\n","04/24/2021 23:02:31 - INFO - __main__ -   Deleting older checkpoint [output-small/checkpoint-80000] due to args.save_total_limit\n","04/24/2021 23:02:35 - INFO - __main__ -   Saving optimizer and scheduler states to output-small/checkpoint-90000\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8cd1bfdddccb40ecb4ae44a22189a185","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Iteration', max=23482.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["04/24/2021 23:19:33 - INFO - __main__ -   Saving model checkpoint to output-small/checkpoint-100000\n","04/24/2021 23:19:33 - INFO - __main__ -   Deleting older checkpoint [output-small/checkpoint-90000] due to args.save_total_limit\n","04/24/2021 23:19:37 - INFO - __main__ -   Saving optimizer and scheduler states to output-small/checkpoint-100000\n","04/24/2021 23:36:35 - INFO - __main__ -   Saving model checkpoint to output-small/checkpoint-110000\n","04/24/2021 23:36:35 - INFO - __main__ -   Deleting older checkpoint [output-small/checkpoint-100000] due to args.save_total_limit\n","04/24/2021 23:36:39 - INFO - __main__ -   Saving optimizer and scheduler states to output-small/checkpoint-110000\n","04/24/2021 23:49:07 - INFO - __main__ -    global_step = 117400, average loss = 1.984183690319669\n","04/24/2021 23:49:07 - INFO - __main__ -   Saving model checkpoint to output-small\n"],"name":"stderr"},{"output_type":"stream","text":["\n","\n"],"name":"stdout"},{"output_type":"stream","text":["04/24/2021 23:49:16 - INFO - __main__ -   Evaluate the following checkpoints: ['output-small']\n","04/24/2021 23:49:21 - INFO - __main__ -   Creating features from dataset file at cached\n","04/24/2021 23:49:27 - INFO - __main__ -   Saving features into cached file cached/gpt2_cached_lm_512\n","04/24/2021 23:49:27 - INFO - __main__ -   ***** Running evaluation  *****\n","04/24/2021 23:49:27 - INFO - __main__ -     Num examples = 5871\n","04/24/2021 23:49:27 - INFO - __main__ -     Batch size = 1\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"051194706137404d9901ccd5bbd8f22c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=5871.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["04/24/2021 23:51:00 - INFO - __main__ -   ***** Eval results  *****\n","04/24/2021 23:51:00 - INFO - __main__ -     perplexity = tensor(1.1446)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["{'perplexity_': tensor(1.1446)}"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"6eDkPEuvbD47"},"source":["# 4. CHAT WITH THE BOT\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":682,"referenced_widgets":["7a216ce9c8fc4ba9a35eafa52a1f71f5","650a8c352904453a9a0c9dc36f416742","e3a475ecc9f54665808defa38734f14f","6f60dd2e60b14ecab30b3f1ad4ccb378","24a54595d6794032bb174a2a149b6a65","d8d609d7a2a948c39629929f7c468383","4dfeff24009043efa8ca6a5bc490fa7b","e5976409fdd44aabb127dddf5cf0c22f","8d1b0e3e436747e89d6d36c2aa2529e3","eb97b859a0984e2aa2c1c4ed45fe2344","cbf53a556e0d4ebfaf3d2ebd202ed113","1dc72c6435234d3a9e6e74efecd9ce17","950de89e805f4793a28595bb55e6e4e5","88545d8240e2434cace9d502fc715770","23d52bce259847c091f3eefdcd3d91db","6a28136ac2184ace8484d29bc8cb4e9a","06b3f70fae754f88b23a90d494bb6a8d","bacba064088d46f6bbf32b5f77ada358","022e099575264c5bafc9f29768eca26c","914a33b2584b4806a6130b5e50040980","d50cff56c02742d1ba2c5dcc455eac66","0c4266003fbb433da5fbce4036d335f1","4d55ab5c301c488d847b48d5a4c4406b","c60d1025c0c147c58d2a8ff5d9c6a8fd","a41abc34852f4a77804cb83bfe93c163","fbd4652998ec402fad312d58fd4ed868","7e73facec03245ffb29dae52d60ac5c7","8cb607fceb824e45aa07294e2c945ddb","e33b96c69f1345bfae9a0829c47d1868","0c8dcb673d7349029d5402ecc01fcd85","4a81a436d1b040329c88d514f993851a","e1fcd4d947fb450986e3caa4614df2f4"]},"id":"nIeqMwZktv7N","executionInfo":{"status":"ok","timestamp":1619309100923,"user_tz":-480,"elapsed":194128,"user":{"displayName":"Taro Bong","photoUrl":"","userId":"11607109179229239372"}},"outputId":"4cb6d340-8850-4467-c241-214e77704903"},"source":["tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n","model = AutoModelForCausalLM.from_pretrained('output-small') # model to use for chatting\n","\n","for step in range(10):\n","    # encode the new user input, add the eos_token and return a tensor in Pytorch\n","    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n","    # print(new_user_input_ids)\n","\n","    # append the new user input tokens to the chat history\n","    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n","\n","    # generated a response while limiting the total chat history to 1000 tokens, \n","    chat_history_ids = model.generate(\n","        bot_input_ids, max_length=500,\n","        pad_token_id=tokenizer.eos_token_id,  \n","        no_repeat_ngram_size=3,       \n","        do_sample=True, \n","        top_k=100, \n","        top_p=0.7,\n","        temperature = 0.8 # adjust this value for more variation\n","    )\n","    \n","    # pretty print last ouput tokens from bot\n","    print(\"Beng Bot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["04/25/2021 00:01:48 - INFO - filelock -   Lock 140070743583632 acquired on /root/.cache/huggingface/transformers/0cbdd50f204f3ddbaa452e976340a5725f0b5ddb201704058c87e14d9679e070.e6898db50ba3aa698f0f652e876a1e4bd813321dea3e22b776f9a3c39d36aaab.lock\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7a216ce9c8fc4ba9a35eafa52a1f71f5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=641.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["04/25/2021 00:01:48 - INFO - filelock -   Lock 140070743583632 released on /root/.cache/huggingface/transformers/0cbdd50f204f3ddbaa452e976340a5725f0b5ddb201704058c87e14d9679e070.e6898db50ba3aa698f0f652e876a1e4bd813321dea3e22b776f9a3c39d36aaab.lock\n","04/25/2021 00:01:48 - INFO - filelock -   Lock 140068417931536 acquired on /root/.cache/huggingface/transformers/3cf340c89a43b5e6f31c4cd609fc2fc92f3d7aafdf6c8987e2ea9e02cb78b4e2.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f.lock\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8d1b0e3e436747e89d6d36c2aa2529e3","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["04/25/2021 00:01:48 - INFO - filelock -   Lock 140068417931536 released on /root/.cache/huggingface/transformers/3cf340c89a43b5e6f31c4cd609fc2fc92f3d7aafdf6c8987e2ea9e02cb78b4e2.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f.lock\n","04/25/2021 00:01:48 - INFO - filelock -   Lock 140073378729104 acquired on /root/.cache/huggingface/transformers/4e3f74e7c741909c4d1b48a23febe75c1be66a20c2b98cf7db4b8b10f12dc10c.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"06b3f70fae754f88b23a90d494bb6a8d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["04/25/2021 00:01:48 - INFO - filelock -   Lock 140073378729104 released on /root/.cache/huggingface/transformers/4e3f74e7c741909c4d1b48a23febe75c1be66a20c2b98cf7db4b8b10f12dc10c.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n","04/25/2021 00:01:48 - INFO - filelock -   Lock 140073378729104 acquired on /root/.cache/huggingface/transformers/5f8cf488e0bdda2e393e798f478673a4d26c1386082a1a05e42269f3ecc89f50.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8.lock\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a41abc34852f4a77804cb83bfe93c163","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=26.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["04/25/2021 00:01:48 - INFO - filelock -   Lock 140073378729104 released on /root/.cache/huggingface/transformers/5f8cf488e0bdda2e393e798f478673a4d26c1386082a1a05e42269f3ecc89f50.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8.lock\n"],"name":"stderr"},{"output_type":"stream","text":["\n",">> User:Oi, later meet bukit batok 6\n","Beng Bot: will be late think i ll eat dinner in school\n",">> User:then liddat meet 8 at the arcade?\n","Beng Bot: no rush going to be l\n",">> User:wat u mean no rush, u the one who wanted to go arcade one leh\n","Beng Bot: u there taking photo\n",">> User:ya i taking photo your taiji meh\n","Beng Bot: heee thanxoxo\n",">> User:thank ur head ar, so how meet 8 at arcade isit\n","Beng Bot: u went offline ar\n",">> User:i as online as it gets liao bro\n","Beng Bot: s ex la he said jus plan stay at e beach nothing else planned yet then u think need bring clothes chang\n",">> User:of course lar, u want smelly smelly take mrt meh\n","Beng Bot: haha okok gd gd luck 4r i got home le i use bd\n",">> User:i thot u still at school??\n","Beng Bot: ya lor haha best is ben 2nd\n",">> User:ben gonna smack u also if u dont tell us what time meet at the arcade\n","Beng Bot: eh i ask my dad fetching me back to lot one then i take bus heh\n",">> User:so isit got time to dinner or not\n","Beng Bot:  if u going ikea str wher\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2nY7P3cfl-0X"},"source":[""],"execution_count":null,"outputs":[]}]}